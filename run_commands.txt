# ------ For overfitting  --------------------------
# for 2 GPU on a single node (single machine)
torchrun --standalone --nproc_per_node=2 model/GPT2.py --num_iterations=200 --sequence_length=1024 --batch_size=16 --total_batch_size=65536  --compile=1 --tensorcores=1 --dtype=bfloat16 --flash=1 --input_train_bin="data/tinyshakespare/*_train.bin"  --output_dir="./outdir"

# for 2 GPU on a single node (single machine) , to load from checkpoint
torchrun --standalone --nproc_per_node=2 model/GPT2.py --num_iterations=200 --sequence_length=1024 --batch_size=16 --total_batch_size=65536  --compile=1 --tensorcores=1 --dtype=bfloat16 --flash=1 --input_train_bin="data/tinyshakespare/*_train.bin"  --output_dir="./outdir" --load_chkpoint_file="./outdir/chkpoint_100.pth"

# ---------------------------------------------------

# -------- For Training on 10BT fineweb edu
# say B=16, grad_acc = 8, GPUs=2, seq_len=1024
# one pass will be : 2*8*16*1024 = 262144 tokens in 1 num_iterations
# we have 10BT corpus, so we need num_iters = 38146 iterations
# total_batch_size = grad_acc*GPUs*BS*Seq_len (tokens)

torchrun --standalone --nproc_per_node=2 model/GPT2.py --num_iterations=38146 --sequence_length=1024 --batch_size=16 --total_batch_size=262144  --compile=1 --tensorcores=1 --dtype=bfloat16 --flash=1 --input_train_bin="data/edu_fineweb10B/*_train_*.bin"  --input_val_bin="data/edu_fineweb10B/*_val_*.bin"  --output_dir="./outdir/edufineweb10B" --learning_rate=6e-4 --weight_decay=0.1 --learning_rate_decay_frac=0.1 --warmup_iters=700 --overfit_single_batch=0 --val_loss_every=100 --sample_every=1000 --zero_stage=0 --save_every_niter=2000 --wandb_log_iters=1 --wandb_project_id="gpt2-10BT"
